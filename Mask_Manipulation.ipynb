{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picklify(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def unpicklify(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = unpicklify('./dumps/lt/fc5/mnist/lt_mask_26.4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 784)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(10, 256)\n"
     ]
    }
   ],
   "source": [
    "for m in mask:\n",
    "    print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model = torch.load('./saves/fc5/mnist/initial_state_dict_lt.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fc5(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0082, -0.0284,  0.0101,  ...,  0.0559, -0.0141, -0.0623],\n",
       "        [ 0.0074,  0.0714,  0.0041,  ..., -0.0214, -0.0116, -0.0516],\n",
       "        [ 0.0163,  0.0478,  0.0005,  ..., -0.0298,  0.0102,  0.0261],\n",
       "        ...,\n",
       "        [-0.0312, -0.0247,  0.0187,  ..., -0.0133, -0.0078, -0.0139],\n",
       "        [ 0.0156,  0.0035, -0.0214,  ..., -0.0427,  0.0868, -0.0070],\n",
       "        [ 0.0508,  0.0040, -0.1378,  ..., -0.0457, -0.0197,  0.0103]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_model.classifier[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sd = init_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(init_sd, 'mlp5_mnist_init_state_dict.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sd = OrderedDict()\n",
    "for k,v in sd.items():\n",
    "    i, name = k.split('.')[1:]\n",
    "    i = int(i)\n",
    "    new_name = f'{i+1}.{name}'\n",
    "    new_sd[new_name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_sd, 'mlp5_mnist_init_state_dict_for_synflow_repo.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_weight_mask(dst_path, weight_src, mask_src, transpose=False):\n",
    "    m_w = torch.load(weight_src)\n",
    "    m_m = torch.load(mask_src)\n",
    "    new_sd = OrderedDict()\n",
    "    for (k_w, v_w), (k_m, v_m) in zip(m_w.named_parameters(), m_m.named_parameters()):\n",
    "        assert k_w == k_m\n",
    "        mask = (v_m != 0.0).type(v_w.dtype)\n",
    "        masked_w = v_w * mask\n",
    "        if transpose and 'weight' in k_w:\n",
    "            masked_w = masked_w.transpose(0,1)\n",
    "        new_sd[k_w] = masked_w\n",
    "        print(masked_w.shape)\n",
    "    torch.save(new_sd, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "combine_weight_mask(\"mlp5_mnist_wt_untrained.pth.tar\", './saves/fc5/mnist/initial_state_dict_lt.pth.tar', './saves/fc5/mnist/9_model_lt.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['classifier.0.weight'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_weight_mask_synflow(dst_path, weight_src, mask_src, transpose=False):\n",
    "    sd_w = torch.load(weight_src)\n",
    "    sd_m = torch.load(mask_src)\n",
    "    new_sd = OrderedDict()\n",
    "    for k in sd_w.keys():\n",
    "        assert k in sd_m.keys()\n",
    "        if k.endswith('mask'):\n",
    "            continue\n",
    "        if k.endswith('weight'):\n",
    "            masked_w = sd_w[k] * sd_m[k+'_mask']\n",
    "            if transpose:\n",
    "                masked_w = masked_w.transpose(0,1)\n",
    "            new_sd[k] = masked_w\n",
    "        else:\n",
    "            new_sd[k] = sd_w[k]\n",
    "        print(new_sd[k].shape)\n",
    "    torch.save(new_sd, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "combine_weight_mask_synflow(\n",
    "    'mlp5_mnist_mag_pai_untrained.pth.tar', '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_mag/singleshot/0/init_model.pt',\n",
    "    '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_mag/singleshot/0/post-train_model.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "combine_weight_mask_synflow(\n",
    "    'mlp5_mnist_synflow_pai_untrained.pth.tar', '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_synflow/singleshot/0/init_model.pt',\n",
    "    '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_synflow/singleshot/0/post-train_model.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "combine_weight_mask_synflow(\n",
    "    'mlp5_mnist_grasp_pai_untrained.pth.tar', '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_grasp/singleshot/0/init_model.pt',\n",
    "    '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_grasp/singleshot/0/post-train_model.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "combine_weight_mask_synflow(\n",
    "    'mlp5_mnist_snip_pai_untrained.pth.tar', '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_snip/singleshot/0/init_model.pt',\n",
    "    '/datadrive_c/xiaohan/network-dl/Synaptic-Flow/mnist/mlp5_snip/singleshot/0/post-train_model.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sd = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_untrained_sd = torch.load('./mlp5_mnist_wt_untrained.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight 86.5787428252551\n",
      "classifier.2.weight 86.57989501953125\n",
      "classifier.4.weight 86.57989501953125\n",
      "classifier.6.weight 86.57989501953125\n",
      "classifier.8.weight 86.6015625\n"
     ]
    }
   ],
   "source": [
    "for k, v in wt_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        v = v.detach().cpu().numpy()\n",
    "        numel = v.size\n",
    "        numz = np.sum(v == 0.0)\n",
    "        sr = numz / numel * 100\n",
    "        print(k, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight\n",
      "classifier.2.weight\n",
      "classifier.4.weight\n",
      "classifier.6.weight\n",
      "classifier.8.weight\n"
     ]
    }
   ],
   "source": [
    "wt_untrained_pckl = []\n",
    "for k, v in wt_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        print(k)\n",
    "        wt_untrained_pckl.append(v.transpose(0,1).detach().cpu().numpy())\n",
    "picklify('random_pruning_exp_pckl_files/mlp5_mnist_wt_untrained.pckl', wt_untrained_pckl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_pai_untrained_sd = torch.load('./mlp5_mnist_mag_pai_untrained.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight 86.57774633290816\n",
      "3.weight 86.57684326171875\n",
      "5.weight 86.57684326171875\n",
      "7.weight 86.57684326171875\n",
      "9.weight 86.5625\n"
     ]
    }
   ],
   "source": [
    "for k, v in mag_pai_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        v = v.detach().cpu().numpy()\n",
    "        numel = v.size\n",
    "        numz = np.sum(v == 0.0)\n",
    "        sr = numz / numel * 100\n",
    "        print(k, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight\n",
      "3.weight\n",
      "5.weight\n",
      "7.weight\n",
      "9.weight\n"
     ]
    }
   ],
   "source": [
    "mag_pai_untrained_pckl = []\n",
    "for k, v in mag_pai_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        print(k)\n",
    "        mag_pai_untrained_pckl.append(v.transpose(0,1).detach().cpu().numpy())\n",
    "picklify('random_pruning_exp_pckl_files/mlp5_mnist_mag_pai_untrained.pckl', mag_pai_untrained_pckl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "classifier.8.weight\n",
      "classifier.8.bias\n"
     ]
    }
   ],
   "source": [
    "new_mag_pai_untrained_sd = OrderedDict()\n",
    "for k, v in mag_pai_untrained_sd.items():\n",
    "    i, name = k.split('.')\n",
    "    new_name = f'classifier.{int(i)-1}.{name}'\n",
    "    print(new_name)\n",
    "    new_mag_pai_untrained_sd[new_name] = v.to(0)\n",
    "torch.save(new_mag_pai_untrained_sd, './random_pruning_exp_state_dict_files/mlp5_mnist_mag_pai_untrained.pth.tar')\n",
    "# picklify('random_pruning_exp_pckl_files/mlp5_mnist_mag_pai_untrained.pckl', mag_pai_untrained_pckl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight 86.57774633290816\n",
      "3.weight 86.57684326171875\n",
      "5.weight 86.57684326171875\n",
      "7.weight 86.57684326171875\n",
      "9.weight 86.5625\n",
      "1.weight\n",
      "3.weight\n",
      "5.weight\n",
      "7.weight\n",
      "9.weight\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.2.weight\n",
      "classifier.2.bias\n",
      "classifier.4.weight\n",
      "classifier.4.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "classifier.8.weight\n",
      "classifier.8.bias\n"
     ]
    }
   ],
   "source": [
    "synflow_pai_untrained_sd = torch.load('./mlp5_mnist_synflow_pai_untrained_from_synflow.pth.tar')\n",
    "\n",
    "for k, v in synflow_pai_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        v = v.detach().cpu().numpy()\n",
    "        numel = v.size\n",
    "        numz = np.sum(v == 0.0)\n",
    "        sr = numz / numel * 100\n",
    "        print(k, sr)\n",
    "\n",
    "synflow_pai_untrained_pckl = []\n",
    "for k, v in synflow_pai_untrained_sd.items():\n",
    "    if 'weight' in k:\n",
    "        print(k)\n",
    "        synflow_pai_untrained_pckl.append(v.transpose(0,1).detach().cpu().numpy())\n",
    "picklify('random_pruning_exp_pckl_files/mlp5_mnist_synflow_pai_untrained.pckl', synflow_pai_untrained_pckl)\n",
    "\n",
    "new_synflow_pai_untrained_sd = OrderedDict()\n",
    "for k, v in synflow_pai_untrained_sd.items():\n",
    "    i, name = k.split('.')\n",
    "    new_name = f'classifier.{int(i)-1}.{name}'\n",
    "    print(new_name)\n",
    "    new_synflow_pai_untrained_sd[new_name] = v.to(0)\n",
    "torch.save(new_synflow_pai_untrained_sd, './random_pruning_exp_state_dict_files/mlp5_mnist_synflow_pai_untrained.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Pruning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sd = torch.load('./mlp5_mnist_init_state_dict.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_sd = torch.load('./mlp5_mnist_wt_untrained.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  ...,  0.0559, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0868, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1378,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_sd['classifier.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0082, -0.0284,  0.0101,  ...,  0.0559, -0.0141, -0.0623],\n",
       "        [ 0.0074,  0.0714,  0.0041,  ..., -0.0214, -0.0116, -0.0516],\n",
       "        [ 0.0163,  0.0478,  0.0005,  ..., -0.0298,  0.0102,  0.0261],\n",
       "        ...,\n",
       "        [-0.0312, -0.0247,  0.0187,  ..., -0.0133, -0.0078, -0.0139],\n",
       "        [ 0.0156,  0.0035, -0.0214,  ..., -0.0427,  0.0868, -0.0070],\n",
       "        [ 0.0508,  0.0040, -0.1378,  ..., -0.0457, -0.0197,  0.0103]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_sd['classifier.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0_sd = torch.load('./random_pruning_alpha_exp_state_dict_files/mlp5_mnist_rp_alpha0.0_seed1_untrained.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  ...,  0.0559, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0868, -0.0000],\n",
       "        [ 0.0000,  0.0000, -0.1378,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha0_sd['classifier.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['classifier.0.weight', 'classifier.0.bias', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias', 'classifier.8.weight', 'classifier.8.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_prune_mlp_from_init_sd(init_sd, pruning_ratio, seed=42):\n",
    "    np.random.seed(seed=seed)\n",
    "    new_sd = OrderedDict()\n",
    "    new_pckl = []\n",
    "    for k, v in init_sd.items():\n",
    "        if 'weight' in k:\n",
    "            numel = v.numel()\n",
    "            num_pruned = int(numel * pruning_ratio)\n",
    "            mask = np.ones(numel).astype(np.float32)\n",
    "            perm = np.random.permutation(numel)\n",
    "            zero_idx = perm[:num_pruned]\n",
    "            mask[zero_idx] = 0.0\n",
    "            mask_t = v.new_tensor(mask).reshape_as(v)\n",
    "            masked_v = v * mask_t\n",
    "            new_pckl.append(masked_v.transpose(0,1).detach().cpu().numpy())\n",
    "            new_sd[k] = masked_v\n",
    "        else:\n",
    "            new_sd[k] = v\n",
    "    return new_sd, new_pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_prune_mlp_alpha_from_init_sd(init_sd, sparse_sd, pruning_ratio, alpha=1.0, seed=42):\n",
    "    np.random.seed(seed=seed)\n",
    "    new_sd = OrderedDict()\n",
    "    new_pckl = []\n",
    "    for k, v in init_sd.items():\n",
    "        if 'weight' in k:\n",
    "            assert k in sparse_sd\n",
    "\n",
    "            v_init_np = v.detach().cpu().numpy()\n",
    "            v_sparse_np = sparse_sd[k].detach().cpu().numpy()\n",
    "            assert v_init_np.size == v_sparse_np.size\n",
    "            \n",
    "            numel = v.numel()\n",
    "            zero_idx_v = (v_sparse_np == 0.0)\n",
    "            \n",
    "            score = np.random.rand(*v_init_np.shape)\n",
    "            score[zero_idx_v] *= alpha\n",
    "            \n",
    "            percentile_value = np.quantile(score, pruning_ratio)\n",
    "            \n",
    "            mask = np.ones_like(v_init_np).astype(np.float32)\n",
    "            mask = np.where(score <= percentile_value, 0.0, mask)\n",
    "            \n",
    "            mask_t = v.new_tensor(mask)\n",
    "            masked_v = v * mask_t\n",
    "            new_pckl.append(masked_v.transpose(0,1).detach().cpu().numpy())\n",
    "            new_sd[k] = masked_v\n",
    "        else:\n",
    "            new_sd[k] = v\n",
    "    return new_sd, new_pckl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[10, 7, 4], [3, 2, 1]])\n",
    "np.percentile(a, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_sd, rp_pckl = random_prune_mlp_from_init_sd(init_sd, 0.8657774633290816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight 86.57774633290816\n",
      "classifier.2.weight 86.57684326171875\n",
      "classifier.4.weight 86.57684326171875\n",
      "classifier.6.weight 86.57684326171875\n",
      "classifier.8.weight 86.5625\n"
     ]
    }
   ],
   "source": [
    "for k, v in rp_sd.items():\n",
    "    if 'weight' in k:\n",
    "        v = v.detach().cpu().numpy()\n",
    "        numel = v.size\n",
    "        numz = np.sum(v == 0.0)\n",
    "        sr = numz / numel * 100\n",
    "        print(k, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 10)\n"
     ]
    }
   ],
   "source": [
    "for _ in rp_pckl:\n",
    "    print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1, 41):\n",
    "    rp_sd, rp_pckl = random_prune_mlp_from_init_sd(init_sd, 0.8657774633290816, seed=seed)\n",
    "    torch.save(rp_sd, f'./random_pruning_exp_state_dict_files/mlp5_mnist_rp_seed{seed}_untrained.pth.tar')\n",
    "    picklify(f'./random_pruning_exp_pckl_files/mlp5_mnist_rp_seed{seed}_untrained.pckl', rp_pckl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1, 6):\n",
    "#     for alpha in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "        rp_sd, rp_pckl = random_prune_mlp_alpha_from_init_sd(init_sd, sparse_sd, 0.8657774633290816, alpha=alpha, seed=seed)\n",
    "        torch.save(rp_sd, f'./random_pruning_alpha_exp_state_dict_files/mlp5_mnist_rp_alpha{alpha}_seed{seed}_untrained.pth.tar')\n",
    "        picklify(f'./random_pruning_alpha_exp_pckl_files/mlp5_mnist_rp_alpha{alpha}_seed{seed}_untrained.pckl', rp_pckl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Random Pruning Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dirs = glob.glob('random_pruning_exp_outputs/*') + glob.glob('random_pruning_alpha_exp_outputs/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_pruning_exp_outputs/mlp5_mnist_rp_seed4_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed31_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed19_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed3_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed12_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed18_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed15_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed10_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed34_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed33_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed40_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_grasp_pai_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed38_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed24_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed21_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed27_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed11_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed32_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_wt_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed30_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed17_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed39_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed26_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed9_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed35_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_snip_pai_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed29_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed23_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed37_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed28_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed36_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed22_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed7_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed20_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed5_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed2_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed8_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed14_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed6_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed13_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed16_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_mag_pai_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed1_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_rp_seed25_untrained',\n",
       " 'random_pruning_exp_outputs/mlp5_mnist_synflow_pai_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.2_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.9_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.7_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.7_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.8_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.1_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.4_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.5_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.0_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.1_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.8_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.3_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.1_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.6_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.4_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.7_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha1.0_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.6_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.5_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.7_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.2_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.6_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.6_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha1.0_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.8_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.2_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.9_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.3_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.1_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.0_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.8_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.6_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.4_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.8_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.0_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.1_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.4_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha1.0_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.5_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.2_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.4_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.0_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.3_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.3_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.9_seed2_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha1.0_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.0_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.5_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha1.0_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.7_seed5_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.9_seed4_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.9_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.2_seed3_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.5_seed1_untrained',\n",
       " 'random_pruning_alpha_exp_outputs/mlp5_mnist_rp_alpha0.3_seed4_untrained']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pruning_acc_results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_dir in exp_dirs:\n",
    "    exp_name = os.path.basename(exp_dir)\n",
    "#     try:\n",
    "    bestacc = unpicklify(os.path.join(exp_dir, 'dumps', 'lt_bestaccuracy.dat'))[0]\n",
    "#     except:\n",
    "#         continue\n",
    "    random_pruning_acc_results[exp_name] = bestacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "picklify('random_pruning_exp_accuracy_results.pckl', random_pruning_acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_pruning_acc_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
